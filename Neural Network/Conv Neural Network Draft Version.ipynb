{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 100\n",
    "# Data size of 500-1000 should typically have a learning rate of 0.001\n",
    "# My test data sample is only 100 so I've used learning rate of 0.0001\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "DATA_PATH = './'\n",
    "MODEL_PATH = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transformation into specific Pytorch Tensor data type used in Pytorch for all various data and weight operations\n",
    "# Compose() normalizes the data into ranges of -1 to 1 or 0 to 1. Means of 0.1307 and Standard deviation of 0.3081\n",
    "# NOTE: Since we're using MNIST dataset that is grayscale (single channel) we do NOT necessarily need to provide MEANS and STD\n",
    "# NOTE: For color images, we need MEANS and STD to be applied to each of the 3 channels (each channel for RGB spectrum)\n",
    "transform_data = torchvision.transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=True, transform=transform_data, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=False, transform=transform_data, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential( # this allows us to create sequentially ordered layers in our network\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2), # input of a single grayscale img, output (channels) of 32,\n",
    "                                                                  # kernel is 5x5 window or filter as it goes through each channel's img features,\n",
    "                                                                  # stride shifts the kernel or window one to the right\n",
    "                                                                  # padding is calculated as 2 with the given input (see docu for eq)\n",
    "            nn.ReLU(), # activation function via Rectified Linear Unit\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) # padding defaults to 0\n",
    "            # the output from layer1 will be 32 channels of 14x14 'images' due to halfing the size from stride = 2\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2), # input of 32 channels from layer1, outputting 64 channels\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            # the output from layer2 will be 64 channels of 7x7 'images' due to halfing the size from stride = 2\n",
    "\n",
    "        self.drop_out = nn.Dropout() # to avoid over-fitting within the model\n",
    "        self.fc1 = nn.Linear(7*7*64, 1000) # input of 7x7 images size multiplied by 64 channels, output 1000\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.reshape(x.size(0), -1) # we MUST flatten the data prior inserting into Linear network.\n",
    "                                     # Flattening data dimensions from 7 x 7 x 64 channels into 3136 x 1\n",
    "        x = self.drop_out(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # NOTE: Softmax is NOT included for Mean Squared Error (MSE) but IS included for Cross Entropy\n",
    "        #meep = nn.Softmax(dim=1)\n",
    "        #return meep(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvNeuralNetwork()\n",
    "\n",
    "# Loss and optimizer\n",
    "#loss = nn.MSELoss()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dataset(net, data):\n",
    "    data_length = len(data)\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i, (images, labels) in enumerate(data):\n",
    "            # Execute forward pass\n",
    "            outputs = net(images)\n",
    "            calc_loss = loss(outputs, labels)\n",
    "            loss_list.append(calc_loss.item()) # calc_loss is still a tensor so to extract the data we must use .item()\n",
    "            \n",
    "            # Back Propragation and Adam Optimizations\n",
    "            optimizer.zero_grad() # zeroing the gradient prior back propragation\n",
    "            calc_loss.backward() # back propragation\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Tracking Accuracy\n",
    "            total = labels.size(0) # EACH EPOCH'S BATCH SIZE WHICH SHOULD BE 100\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1) # torch.max(tensor, axis) returns '_' as max values within each img and 'predicted' as\n",
    "                                                 # max values' index number which correlates to the predicted img number.\n",
    "                                                 # Each in index will represent the number 0-9, totaling 10.\n",
    "                                                 # Remember that the output for each image results will output tensor length of 10 data points.\n",
    "            \n",
    "            correct = (predicted == labels).sum().item() # Checking prediction tensor with label tensor to see if predictions are correct.\n",
    "                                                         # Sum all the correct predictions with .sum() and convert from tensor to int with .item()\n",
    "            \n",
    "            accuracy_list.append(correct / total)\n",
    "            \n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {calc_loss.item()}, Accuracy: {(correct/total) * 100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.277619868516922, Accuracy: 95.0 %\n",
      "Epoch [1/1], Loss: 0.0568741038441658, Accuracy: 99.0 %\n",
      "Epoch [1/1], Loss: 0.14643609523773193, Accuracy: 97.0 %\n",
      "Epoch [1/1], Loss: 0.03588870167732239, Accuracy: 99.0 %\n",
      "Epoch [1/1], Loss: 0.06293945014476776, Accuracy: 98.0 %\n",
      "Epoch [1/1], Loss: 0.07110723853111267, Accuracy: 98.0 %\n"
     ]
    }
   ],
   "source": [
    "train_dataset(net, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset(net, data):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # eval() disables any drop-out o batch normalization layers within our network\n",
    "    net.eval()\n",
    "    \n",
    "    # We do NOT want to train/optimize our data and only test our \n",
    "    # neural network's accuracy, so gradient must not be used!\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data:\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print(total)\n",
    "        print(f'Test Accuracy from 10,000 test images: {(correct / total) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Test Accuracy from 10,000 test images: 98.35000000000001%\n"
     ]
    }
   ],
   "source": [
    "test_dataset(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    IMG_SIZE = 28\n",
    "    DATA_DIR = os.getcwd() + '/Numbers_Test/'\n",
    "    LABELS = [0,1,2,3,4,5,6,7,8,9]\n",
    "    dataset = []\n",
    "    \n",
    "    def create_data(self):\n",
    "        for label in self.LABELS:\n",
    "            data_path = os.path.join(self.DATA_DIR, str(label))\n",
    "            for f in os.listdir(data_path):\n",
    "                try:\n",
    "                    path = os.path.join(data_path, f)\n",
    "                    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                    img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE)) # image is already 28x28 but for usage on any others in future\n",
    "                    self.dataset.append([np.array(img), np.eye(10)[self.LABELS[label]]]) # setting the numpy 'eye' outcomes for each img\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "        \n",
    "        np.random.shuffle(self.dataset)\n",
    "        np.save(self.DATA_DIR + \"NumberImgData.npy\", self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = DataGenerator()\n",
    "test.create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "outer_sample = np.load(\"./Numbers_Test/NumberImgData.npy\", allow_pickle=True)\n",
    "print(len(outer_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_index = 90\n",
    "\n",
    "print(np.argmax(outer_sample[data_index][1]))\n",
    "plt.imshow(outer_sample[data_index][0], cmap=\"gray\") # first index contains the data selection from 0 to n\n",
    "                                            # second index contains the img pixel data [0] and results [1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "X = torch.Tensor([i[0] for i in outer_sample]).view(-1, 28, 28) # separating the img values into the proper format\n",
    "Y = torch.Tensor([(i[1]) for i in outer_sample]) # separating the img answers\n",
    "\n",
    "test_percent = 0.1\n",
    "test_val = int(len(X) * test_percent)\n",
    "\n",
    "train_X = X[:-test_val]\n",
    "train_Y = Y[:-test_val]\n",
    "\n",
    "test_X = X[-test_val:]\n",
    "test_Y = Y[-test_val:]\n",
    "print(len(train_X))\n",
    "print(len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_outer_dataset(net, imgData, imgResults):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # eval() disables any drop-out or batch normalization layers within our network\n",
    "    net.eval()\n",
    "    \n",
    "    # We do NOT want to train/optimize our data and only test our \n",
    "    # neural network's accuracy, so gradient must not be used!\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(imgData)):\n",
    "            batch_X = imgData[i].view(-1, 1, 28, 28)\n",
    "            # For Cross Entropy Loss, you must have all the batch's number results in a tensor\n",
    "            batch_Y = torch.argmax(imgResults[i])\n",
    "            # Whereas for Mean Standard Error (MSE) Loss, you must have the batch's number results in a tensor AS a ONE HOT VECTOR\n",
    "            # batch_Y = imgResults[i:i + BATCH_SIZE] # This format is already in a one HOT VECTOR format\n",
    "            number_answer = torch.argmax(imgResults[i])\n",
    "            outputs = net(batch_X)\n",
    "            predicted_num = torch.argmax(outputs).item()\n",
    "            print(f'predicted: {predicted_num}')\n",
    "            print(f'answer: {number_answer}')\n",
    "            print('------')\n",
    "            #print(outputs)\n",
    "            if predicted_num == number_answer:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            \n",
    "        print(f'Total: {total}')\n",
    "        print(f'Test Accuracy from outside images samples: {(correct / total) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: 5\n",
      "answer: 5\n",
      "------\n",
      "predicted: 3\n",
      "answer: 8\n",
      "------\n",
      "predicted: 0\n",
      "answer: 0\n",
      "------\n",
      "predicted: 5\n",
      "answer: 5\n",
      "------\n",
      "predicted: 6\n",
      "answer: 6\n",
      "------\n",
      "predicted: 1\n",
      "answer: 1\n",
      "------\n",
      "predicted: 8\n",
      "answer: 8\n",
      "------\n",
      "predicted: 9\n",
      "answer: 9\n",
      "------\n",
      "predicted: 8\n",
      "answer: 8\n",
      "------\n",
      "predicted: 3\n",
      "answer: 3\n",
      "------\n",
      "predicted: 8\n",
      "answer: 0\n",
      "------\n",
      "predicted: 4\n",
      "answer: 4\n",
      "------\n",
      "predicted: 2\n",
      "answer: 2\n",
      "------\n",
      "predicted: 9\n",
      "answer: 9\n",
      "------\n",
      "predicted: 4\n",
      "answer: 4\n",
      "------\n",
      "predicted: 9\n",
      "answer: 9\n",
      "------\n",
      "predicted: 3\n",
      "answer: 3\n",
      "------\n",
      "predicted: 4\n",
      "answer: 1\n",
      "------\n",
      "predicted: 7\n",
      "answer: 9\n",
      "------\n",
      "predicted: 6\n",
      "answer: 6\n",
      "------\n",
      "predicted: 3\n",
      "answer: 3\n",
      "------\n",
      "predicted: 5\n",
      "answer: 5\n",
      "------\n",
      "predicted: 5\n",
      "answer: 5\n",
      "------\n",
      "predicted: 4\n",
      "answer: 4\n",
      "------\n",
      "predicted: 3\n",
      "answer: 7\n",
      "------\n",
      "predicted: 7\n",
      "answer: 2\n",
      "------\n",
      "predicted: 2\n",
      "answer: 2\n",
      "------\n",
      "predicted: 9\n",
      "answer: 9\n",
      "------\n",
      "predicted: 2\n",
      "answer: 2\n",
      "------\n",
      "predicted: 3\n",
      "answer: 7\n",
      "------\n",
      "predicted: 0\n",
      "answer: 0\n",
      "------\n",
      "predicted: 7\n",
      "answer: 7\n",
      "------\n",
      "predicted: 6\n",
      "answer: 6\n",
      "------\n",
      "predicted: 6\n",
      "answer: 6\n",
      "------\n",
      "predicted: 0\n",
      "answer: 0\n",
      "------\n",
      "predicted: 8\n",
      "answer: 8\n",
      "------\n",
      "predicted: 3\n",
      "answer: 7\n",
      "------\n",
      "predicted: 7\n",
      "answer: 7\n",
      "------\n",
      "predicted: 9\n",
      "answer: 9\n",
      "------\n",
      "predicted: 3\n",
      "answer: 3\n",
      "------\n",
      "predicted: 1\n",
      "answer: 1\n",
      "------\n",
      "predicted: 3\n",
      "answer: 3\n",
      "------\n",
      "predicted: 2\n",
      "answer: 2\n",
      "------\n",
      "predicted: 3\n",
      "answer: 7\n",
      "------\n",
      "predicted: 6\n",
      "answer: 6\n",
      "------\n",
      "predicted: 5\n",
      "answer: 5\n",
      "------\n",
      "predicted: 6\n",
      "answer: 8\n",
      "------\n",
      "predicted: 0\n",
      "answer: 0\n",
      "------\n",
      "predicted: 3\n",
      "answer: 3\n",
      "------\n",
      "predicted: 5\n",
      "answer: 5\n",
      "------\n",
      "predicted: 2\n",
      "answer: 2\n",
      "------\n",
      "predicted: 0\n",
      "answer: 0\n",
      "------\n",
      "predicted: 8\n",
      "answer: 8\n",
      "------\n",
      "predicted: 3\n",
      "answer: 3\n",
      "------\n",
      "predicted: 0\n",
      "answer: 0\n",
      "------\n",
      "predicted: 8\n",
      "answer: 8\n",
      "------\n",
      "predicted: 4\n",
      "answer: 4\n",
      "------\n",
      "predicted: 7\n",
      "answer: 7\n",
      "------\n",
      "predicted: 5\n",
      "answer: 5\n",
      "------\n",
      "predicted: 6\n",
      "answer: 8\n",
      "------\n",
      "predicted: 1\n",
      "answer: 1\n",
      "------\n",
      "predicted: 2\n",
      "answer: 2\n",
      "------\n",
      "predicted: 3\n",
      "answer: 3\n",
      "------\n",
      "predicted: 5\n",
      "answer: 5\n",
      "------\n",
      "predicted: 1\n",
      "answer: 1\n",
      "------\n",
      "predicted: 6\n",
      "answer: 6\n",
      "------\n",
      "predicted: 9\n",
      "answer: 9\n",
      "------\n",
      "predicted: 4\n",
      "answer: 9\n",
      "------\n",
      "predicted: 6\n",
      "answer: 6\n",
      "------\n",
      "predicted: 1\n",
      "answer: 1\n",
      "------\n",
      "predicted: 2\n",
      "answer: 2\n",
      "------\n",
      "predicted: 6\n",
      "answer: 8\n",
      "------\n",
      "predicted: 1\n",
      "answer: 4\n",
      "------\n",
      "predicted: 6\n",
      "answer: 8\n",
      "------\n",
      "predicted: 9\n",
      "answer: 9\n",
      "------\n",
      "predicted: 6\n",
      "answer: 6\n",
      "------\n",
      "predicted: 3\n",
      "answer: 7\n",
      "------\n",
      "predicted: 1\n",
      "answer: 1\n",
      "------\n",
      "predicted: 9\n",
      "answer: 9\n",
      "------\n",
      "predicted: 5\n",
      "answer: 5\n",
      "------\n",
      "predicted: 4\n",
      "answer: 4\n",
      "------\n",
      "predicted: 2\n",
      "answer: 2\n",
      "------\n",
      "predicted: 3\n",
      "answer: 3\n",
      "------\n",
      "predicted: 6\n",
      "answer: 6\n",
      "------\n",
      "predicted: 3\n",
      "answer: 3\n",
      "------\n",
      "predicted: 2\n",
      "answer: 2\n",
      "------\n",
      "predicted: 1\n",
      "answer: 1\n",
      "------\n",
      "predicted: 2\n",
      "answer: 1\n",
      "------\n",
      "predicted: 5\n",
      "answer: 5\n",
      "------\n",
      "predicted: 7\n",
      "answer: 7\n",
      "------\n",
      "Total: 90\n",
      "Test Accuracy from outside images samples: 81.11111111111111%\n"
     ]
    }
   ],
   "source": [
    "test_outer_dataset(net, train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_outer_dataset(net, imgData, imgResults):\n",
    "    EPOCHS = 20\n",
    "    BATCH_SIZE = 5\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in range(0, len(imgData), BATCH_SIZE): # splitting up all the data into their respective batch size using 'range'\n",
    "            batch_X = imgData[i:i + BATCH_SIZE].view(-1, 1, 28, 28)\n",
    "            # For Cross Entropy Loss, you must have all the batch's number results in a tensor\n",
    "            batch_Y = torch.argmax(imgResults[i:i + BATCH_SIZE], 1)\n",
    "            # Whereas for Mean Standard Error (MSE) Loss, you must have the batch's number results in a tensor AS a ONE HOT VECTOR\n",
    "            # batch_Y = imgResults[i:i + BATCH_SIZE] # This format is already in a one HOT VECTOR format\n",
    "            net.zero_grad()\n",
    "            outputs = net(batch_X)\n",
    "            calc_loss = loss(outputs, batch_Y)\n",
    "            calc_loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch: {epoch}. Loss: {calc_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Loss: 1.4305112472356996e-07\n",
      "Epoch: 1. Loss: 1.4305112472356996e-07\n",
      "Epoch: 2. Loss: 1.4305112472356996e-07\n",
      "Epoch: 3. Loss: 1.4305112472356996e-07\n",
      "Epoch: 4. Loss: 1.4305112472356996e-07\n",
      "Epoch: 5. Loss: 1.4305112472356996e-07\n",
      "Epoch: 6. Loss: 1.4305112472356996e-07\n",
      "Epoch: 7. Loss: 1.4305112472356996e-07\n",
      "Epoch: 8. Loss: 1.1920926823449918e-07\n",
      "Epoch: 9. Loss: 1.1920926823449918e-07\n",
      "Epoch: 10. Loss: 1.1920926823449918e-07\n",
      "Epoch: 11. Loss: 1.1920926823449918e-07\n",
      "Epoch: 12. Loss: 1.4305111051271524e-07\n",
      "Epoch: 13. Loss: 1.4305111051271524e-07\n",
      "Epoch: 14. Loss: 1.4305111051271524e-07\n",
      "Epoch: 15. Loss: 1.4305111051271524e-07\n",
      "Epoch: 16. Loss: 1.4305111051271524e-07\n",
      "Epoch: 17. Loss: 1.4305111051271524e-07\n",
      "Epoch: 18. Loss: 1.4305111051271524e-07\n",
      "Epoch: 19. Loss: 1.6689295989635866e-07\n"
     ]
    }
   ],
   "source": [
    "train_outer_dataset(net, train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
