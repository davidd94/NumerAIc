{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 100\n",
    "# Data size of 500-1000 should typically have a learning rate of 0.001\n",
    "# My test data sample is only 100 so I've used learning rate of 0.0001\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "DATA_PATH = './'\n",
    "MODEL_PATH = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transformation into specific Pytorch Tensor data type used in Pytorch for all various data and weight operations\n",
    "# Compose() normalizes the data into ranges of -1 to 1 or 0 to 1. Means of 0.1307 and Standard deviation of 0.3081\n",
    "# NOTE: Since we're using MNIST dataset that is grayscale (single channel) we do NOT necessarily need to provide MEANS and STD\n",
    "# NOTE: For color images, we need MEANS and STD to be applied to each of the 3 channels (each channel for RGB spectrum)\n",
    "transform_data = torchvision.transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=True, transform=transform_data, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=False, transform=transform_data, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential( # this allows us to create sequentially ordered layers in our network\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2), # input of a single grayscale img, output (channels) of 32,\n",
    "                                                                  # kernel is 5x5 window or filter as it goes through each channel's img features,\n",
    "                                                                  # stride shifts the kernel or window one to the right\n",
    "                                                                  # padding is calculated as 2 with the given input (see docu for eq)\n",
    "            nn.ReLU(), # activation function via Rectified Linear Unit\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) # padding defaults to 0\n",
    "            # the output from layer1 will be 32 channels of 14x14 'images' due to halfing the size from stride = 2\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2), # input of 32 channels from layer1, outputting 64 channels\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            # the output from layer2 will be 64 channels of 7x7 'images' due to halfing the size from stride = 2\n",
    "\n",
    "        self.drop_out = nn.Dropout() # to avoid over-fitting within the model\n",
    "        self.fc1 = nn.Linear(7*7*64, 1000) # input of 7x7 images size multiplied by 64 channels, output 1000\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.reshape(x.size(0), -1) # we MUST flatten the data prior inserting into Linear network.\n",
    "                                     # Flattening data dimensions from 7 x 7 x 64 channels into 3136 x 1\n",
    "        x = self.drop_out(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # NOTE: Softmax is NOT included for Mean Squared Error (MSE) but IS included for Cross Entropy\n",
    "        #meep = nn.Softmax(dim=1)\n",
    "        #return meep(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvNeuralNetwork()\n",
    "\n",
    "# Loss and optimizer\n",
    "#loss = nn.MSELoss()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dataset(net, data):\n",
    "    data_length = len(data)\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i, (images, labels) in enumerate(data):\n",
    "            # Execute forward pass\n",
    "            outputs = net(images)\n",
    "            calc_loss = loss(outputs, labels)\n",
    "            loss_list.append(calc_loss.item()) # calc_loss is still a tensor so to extract the data we must use .item()\n",
    "            \n",
    "            # Back Propragation and Adam Optimizations\n",
    "            optimizer.zero_grad() # zeroing the gradient prior back propragation\n",
    "            calc_loss.backward() # back propragation\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Tracking Accuracy\n",
    "            total = labels.size(0) # EACH EPOCH'S BATCH SIZE WHICH SHOULD BE 100\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1) # torch.max(tensor, axis) returns '_' as max values within each img and 'predicted' as\n",
    "                                                 # max values' index number which correlates to the predicted img number.\n",
    "                                                 # Each in index will represent the number 0-9, totaling 10.\n",
    "                                                 # Remember that the output for each image results will output tensor length of 10 data points.\n",
    "            \n",
    "            correct = (predicted == labels).sum().item() # Checking prediction tensor with label tensor to see if predictions are correct.\n",
    "                                                         # Sum all the correct predictions with .sum() and convert from tensor to int with .item()\n",
    "            \n",
    "            accuracy_list.append(correct / total)\n",
    "            \n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {calc_loss.item()}, Accuracy: {(correct/total) * 100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.5905557870864868, Accuracy: 86.0 %\n",
      "Epoch [1/1], Loss: 0.4867016077041626, Accuracy: 88.0 %\n",
      "Epoch [1/1], Loss: 0.2925793528556824, Accuracy: 95.0 %\n",
      "Epoch [1/1], Loss: 0.10551241785287857, Accuracy: 98.0 %\n",
      "Epoch [1/1], Loss: 0.1956314593553543, Accuracy: 93.0 %\n",
      "Epoch [1/1], Loss: 0.12617231905460358, Accuracy: 96.0 %\n"
     ]
    }
   ],
   "source": [
    "train_dataset(net, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset(net, data):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # eval() disables any drop-out o batch normalization layers within our network\n",
    "    net.eval()\n",
    "    \n",
    "    # We do NOT want to train/optimize our data and only test our \n",
    "    # neural network's accuracy, so gradient must not be used!\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data:\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print(total)\n",
    "        print(f'Test Accuracy from 10,000 test images: {(correct / total) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Test Accuracy from 10,000 test images: 37.18%\n"
     ]
    }
   ],
   "source": [
    "test_dataset(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    IMG_SIZE = 28\n",
    "    DATA_DIR = os.getcwd() + '/custom_numbers/'\n",
    "    LABELS = [0,1,2,3,4,5,6,7,8,9]\n",
    "    dataset = []\n",
    "    \n",
    "    def create_data(self):\n",
    "        for label in self.LABELS:\n",
    "            data_path = os.path.join(self.DATA_DIR, str(label))\n",
    "            for f in os.listdir(data_path):\n",
    "                try:\n",
    "                    path = os.path.join(data_path, f)\n",
    "                    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                    img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE)) # image is already 28x28 but for usage on any others in future\n",
    "                    self.dataset.append([np.array(img), np.eye(10)[self.LABELS[label]]]) # setting the numpy 'eye' outcomes for each img\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "        \n",
    "        np.random.shuffle(self.dataset)\n",
    "        np.save(self.DATA_DIR + \"NumberImgData.npy\", self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = DataGenerator()\n",
    "test.create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n"
     ]
    }
   ],
   "source": [
    "outer_sample = np.load(\"./custom_numbers/NumberImgData.npy\", allow_pickle=True)\n",
    "print(len(outer_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAL0UlEQVR4nO3dXagc9R3G8efJScT4RpJKwmmMjS25aBGqRUKhUuyFkuYmemHRq5QWjhdaFAptsBcKUpC+XhYiDabFKoJag5RqCNJ4JTmKjYlBk0qqxxxySONLglHz8uvFzpFj3N3Z7Ozs7Dm/7weW3Z3ZnfllNs+Zmf9/dv+OCAFY+BY1XQCA4SDsQBKEHUiCsANJEHYgicXDXJltmv6BmkWE202vtGe3vcH2m7YP2d5SZVkA6uV++9ltj0l6S9LNkqYk7ZF0Z0S80eU97NmBmtWxZ18v6VBEvB0Rn0l6QtKmCssDUKMqYV8t6d05z6eKaV9ge8L2pO3JCusCUFGVBrp2hwpfOkyPiK2StkocxgNNqrJnn5K0Zs7zqyQdqVYOgLpUCfseSetsX2P7Ikl3SNoxmLIADFrfh/ERccb2PZKelzQmaVtE7B9YZQAGqu+ut75Wxjk7ULtaLqoBMH8QdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQxFB/ShrDt3Tp0q7zT506NaRK0DT27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBP3sC8CyZcs6znv//fe7vtdu+0OkWIDYswNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoziusCVfb70sy88nUZxrXRRje3Dkk5IOivpTETcUGV5AOoziCvofhARxwawHAA14pwdSKJq2EPSC7ZfsT3R7gW2J2xP2p6suC4AFVRqoLP91Yg4YnulpJ2SfhYRu7u8nga6IaOBLp9ODXSV9uwRcaS4n5H0jKT1VZYHoD59h932pbYvn30s6RZJ+wZVGIDBqtIav0rSM8Vh4GJJf4uIfw6kKlyQbt9nB2b1HfaIeFvStwdYC4Aa0fUGJEHYgSQIO5AEYQeSIOxAEnzFdQHo9hlyhVw+tVxBB2D+IOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkBjGwI0bYokXd/56fO3duSJWgaezZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJ+tnngbLf9ue34dGL0j277W22Z2zvmzNthe2dtg8W98vrLRNAVb0cxj8qacN507ZI2hUR6yTtKp4DGGGlYY+I3ZKOnzd5k6TtxePtkm4dcF0ABqzfc/ZVETEtSRExbXtlpxfanpA00ed6AAxI7Q10EbFV0laJgR2BJvXb9XbU9rgkFfczgysJQB36DfsOSZuLx5slPTuYcgDUpXR8dtuPS7pJ0pWSjkp6QNLfJT0p6WpJ70i6PSLOb8RrtywO49so+wwWqrGxsa7zlyxZ0nX+p59+OshyFoxO47OXhn2QCHt7hL09wt6fTmHnclkgCcIOJEHYgSQIO5AEYQeSoDV+HpjPX3Gt8v9rlP9do4zWeCA5wg4kQdiBJAg7kARhB5Ig7EAShB1Igp+SHoLMwyZ36ysv64Ofz9cXjCL27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBP3sQ1DWj561P/mSSy7pOv/jjz8eUiU5sGcHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSToZ0etli5d2nEeo7AOV+me3fY22zO2982Z9qDt92y/Vtw21lsmgKp6OYx/VNKGNtP/GBHXFbd/DLYsAINWGvaI2C3p+BBqAVCjKg1099jeWxzmL+/0ItsTtidtT1ZYF4CKehrY0fZaSc9FxLXF81WSjkkKSQ9JGo+In/SwHAZ2bGMhfxGmSgPd2bNnu86fz9ulTgMd2DEijkbE2Yg4J+kRSeurFAegfn2F3fb4nKe3SdrX6bUARkNpP7vtxyXdJOlK21OSHpB0k+3r1DqMPyzprhprnPe6HcrWbfHi7h/xmTNnal3/qVOnOs674ooral03vqinc/aBrSzpOXtZ2Mt+pKHKuWnTYe+mLOwffvhh1/mcs7c30HN2APMPYQeSIOxAEoQdSIKwA0nQGj8Cqn4GF198ccd5TX+NtFtvwOnTpystm9b49miNB5Ij7EAShB1IgrADSRB2IAnCDiRB2IEk+CnpEVDWX1zWD//JJ590nDc2Ntb1vWXDSVdVpS99+fKOv3aGPrBnB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEk6GcfAYsWdf+bW9YP3+39ZaOqNKmsH/2DDz4YUiU5sGcHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSToZx8BZd8pX7ZsWdf5J0+e7Div6m+rl10DUOf34Ztc90JUume3vcb2i7YP2N5v+95i+grbO20fLO75pQFghPVyGH9G0s8j4puSvivpbtvfkrRF0q6IWCdpV/EcwIgqDXtETEfEq8XjE5IOSFotaZOk7cXLtku6ta4iAVR3QefsttdKul7Sy5JWRcS01PqDYHtlh/dMSJqoViaAqnoOu+3LJD0l6b6I+KjXhp+I2Cppa7EMBnYEGtJT15vtJWoF/bGIeLqYfNT2eDF/XNJMPSUCGITSIZvd2oVvl3Q8Iu6bM/23kv4XEQ/b3iJpRUT8omRZ7NmBmnUasrmXsN8o6SVJr0ua7di8X63z9iclXS3pHUm3R8TxkmURdqBmfYd9kAg7UL9OYedyWSAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5IoDbvtNbZftH3A9n7b9xbTH7T9nu3XitvG+ssF0K9exmcflzQeEa/avlzSK5JulfQjSScj4nc9r4whm4HadRqyeXEPb5yWNF08PmH7gKTVgy0PQN0u6Jzd9lpJ10t6uZh0j+29trfZXt7hPRO2J21PVqoUQCWlh/Gfv9C+TNK/JP06Ip62vUrSMUkh6SG1DvV/UrIMDuOBmnU6jO8p7LaXSHpO0vMR8Yc289dKei4iri1ZDmEHatYp7L20xlvSnyUdmBv0ouFu1m2S9lUtEkB9emmNv1HSS5Jel3SumHy/pDslXafWYfxhSXcVjXndlsWeHahZpcP4QSHsQP36PowHsDAQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkij9wckBOybpv3OeX1lMG0WjWtuo1iVRW78GWdvXOs0Y6vfZv7RyezIibmisgC5GtbZRrUuitn4NqzYO44EkCDuQRNNh39rw+rsZ1dpGtS6J2vo1lNoaPWcHMDxN79kBDAlhB5JoJOy2N9h+0/Yh21uaqKET24dtv14MQ93o+HTFGHoztvfNmbbC9k7bB4v7tmPsNVTbSAzj3WWY8Ua3XdPDnw/9nN32mKS3JN0saUrSHkl3RsQbQy2kA9uHJd0QEY1fgGH7+5JOSvrL7NBatn8j6XhEPFz8oVweEb8ckdoe1AUO411TbZ2GGf+xGtx2gxz+vB9N7NnXSzoUEW9HxGeSnpC0qYE6Rl5E7JZ0/LzJmyRtLx5vV+s/y9B1qG0kRMR0RLxaPD4haXaY8Ua3XZe6hqKJsK+W9O6c51MarfHeQ9ILtl+xPdF0MW2smh1mq7hf2XA95ysdxnuYzhtmfGS2XT/Dn1fVRNjbDU0zSv1/34uI70j6oaS7i8NV9OZPkr6h1hiA05J+32QxxTDjT0m6LyI+arKWudrUNZTt1kTYpyStmfP8KklHGqijrYg4UtzPSHpGrdOOUXJ0dgTd4n6m4Xo+FxFHI+JsRJyT9Iga3HbFMONPSXosIp4uJje+7drVNazt1kTY90haZ/sa2xdJukPSjgbq+BLblxYNJ7J9qaRbNHpDUe+QtLl4vFnSsw3W8gWjMox3p2HG1fC2a3z484gY+k3SRrVa5P8j6VdN1NChrq9L+ndx2990bZIeV+uw7rRaR0Q/lfQVSbskHSzuV4xQbX9Va2jvvWoFa7yh2m5U69Rwr6TXitvGprddl7qGst24XBZIgivogCQIO5AEYQeSIOxAEoQdSIKwA0kQdiCJ/wMWZ/509vvYmQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_index = 90\n",
    "\n",
    "print(np.argmax(outer_sample[data_index][1]))\n",
    "plt.imshow(outer_sample[data_index][0], cmap=\"gray\") # first index contains the data selection from 0 to n\n",
    "                                            # second index contains the img pixel data [0] and results [1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "X = torch.Tensor([i[0] for i in outer_sample]).view(-1, 28, 28) # separating the img values into the proper format\n",
    "Y = torch.Tensor([(i[1]) for i in outer_sample]) # separating the img answers\n",
    "\n",
    "test_percent = 0.1\n",
    "test_val = int(len(X) * test_percent)\n",
    "\n",
    "train_X = X[:-test_val]\n",
    "train_Y = Y[:-test_val]\n",
    "\n",
    "test_X = X[-test_val:]\n",
    "test_Y = Y[-test_val:]\n",
    "print(len(train_X))\n",
    "print(len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeuralNetwork(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (drop_out): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=3136, out_features=1000, bias=True)\n",
       "  (fc2): Linear(in_features=1000, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load network model from file\n",
    "model_path = './network_model.pt'\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_outer_dataset(net, imgData, imgResults):\n",
    "    EPOCHS = 5\n",
    "    BATCH_SIZE = 5\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in range(0, len(imgData), BATCH_SIZE): # splitting up all the data into their respective batch size using 'range'\n",
    "            batch_X = imgData[i:i + BATCH_SIZE].view(-1, 1, 28, 28)\n",
    "            # For Cross Entropy Loss, you must have all the batch's number results in a tensor\n",
    "            batch_Y = torch.argmax(imgResults[i:i + BATCH_SIZE], 1)\n",
    "            # Whereas for Mean Standard Error (MSE) Loss, you must have the batch's number results in a tensor AS a ONE HOT VECTOR\n",
    "            # batch_Y = imgResults[i:i + BATCH_SIZE] # This format is already in a one HOT VECTOR format\n",
    "            net.zero_grad()\n",
    "            outputs = net(batch_X)\n",
    "            calc_loss = loss(outputs, batch_Y)\n",
    "            calc_loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch: {epoch}. Loss: {calc_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Loss: 0.0\n",
      "Epoch: 1. Loss: 0.2834509313106537\n",
      "Epoch: 2. Loss: 0.0\n",
      "Epoch: 3. Loss: 0.0\n",
      "Epoch: 4. Loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "train_outer_dataset(net, train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_outer_dataset(net, imgData, imgResults):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # eval() disables any drop-out or batch normalization layers within our network\n",
    "    net.eval()\n",
    "    \n",
    "    # We do NOT want to train/optimize our data and only test our \n",
    "    # neural network's accuracy, so gradient must not be used!\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(imgData)):\n",
    "            batch_X = imgData[i].view(-1, 1, 28, 28)\n",
    "            # For Cross Entropy Loss, you must have all the batch's number results in a tensor\n",
    "            batch_Y = torch.argmax(imgResults[i])\n",
    "            # Whereas for Mean Standard Error (MSE) Loss, you must have the batch's number results in a tensor AS a ONE HOT VECTOR\n",
    "            # batch_Y = imgResults[i:i + BATCH_SIZE] # This format is already in a one HOT VECTOR format\n",
    "            number_answer = torch.argmax(imgResults[i])\n",
    "            outputs = net(batch_X)\n",
    "            predicted_num = torch.argmax(outputs).item()\n",
    "            print(f'predicted: {predicted_num}')\n",
    "            print(f'answer: {number_answer}')\n",
    "            print('------')\n",
    "            #print(outputs)\n",
    "            if predicted_num == number_answer:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            \n",
    "        print(f'Total: {total}')\n",
    "        print(f'Test Accuracy from outside images samples: {(correct / total) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: 8\n",
      "answer: 8\n",
      "------\n",
      "predicted: 1\n",
      "answer: 1\n",
      "------\n",
      "predicted: 4\n",
      "answer: 4\n",
      "------\n",
      "predicted: 7\n",
      "answer: 7\n",
      "------\n",
      "predicted: 1\n",
      "answer: 1\n",
      "------\n",
      "predicted: 9\n",
      "answer: 9\n",
      "------\n",
      "predicted: 7\n",
      "answer: 7\n",
      "------\n",
      "predicted: 2\n",
      "answer: 2\n",
      "------\n",
      "predicted: 8\n",
      "answer: 8\n",
      "------\n",
      "predicted: 0\n",
      "answer: 0\n",
      "------\n",
      "predicted: 4\n",
      "answer: 4\n",
      "------\n",
      "predicted: 7\n",
      "answer: 7\n",
      "------\n",
      "predicted: 0\n",
      "answer: 0\n",
      "------\n",
      "predicted: 9\n",
      "answer: 9\n",
      "------\n",
      "predicted: 2\n",
      "answer: 0\n",
      "------\n",
      "predicted: 9\n",
      "answer: 9\n",
      "------\n",
      "predicted: 4\n",
      "answer: 4\n",
      "------\n",
      "Total: 17\n",
      "Test Accuracy from outside images samples: 94.11764705882352%\n"
     ]
    }
   ],
   "source": [
    "test_outer_dataset(net, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model as file\n",
    "file_name = 'network_model'\n",
    "full_model_path = f'./{file_name}.pt'\n",
    "\n",
    "torch.save(net.state_dict(), full_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
